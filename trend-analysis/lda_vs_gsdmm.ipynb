{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330c5cb9",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c784ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767b6b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from rake_nltk import Rake\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import collections\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "463f541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gsdmm import MovieGroupProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8435828d",
   "metadata": {},
   "source": [
    "## Read Tweets Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb87d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50711dc2",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe31fb",
   "metadata": {},
   "source": [
    "Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75d464a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['user', 'date', 'Tweet Id'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da06e7b",
   "metadata": {},
   "source": [
    "Remove URLs from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b96dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+','', text)\n",
    "\n",
    "df['text'] = df['text'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f107a56",
   "metadata": {},
   "source": [
    "Lowercase all alphabets and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e9b5fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\urjak\\AppData\\Local\\Temp/ipykernel_20572/896317003.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['clean'] = df['text'].str.lower().str.replace('[^\\w\\s]', ' ').str.replace(' +', ' ').str.strip()\n"
     ]
    }
   ],
   "source": [
    "df['clean'] = df['text'].str.lower().str.replace('[^\\w\\s]', ' ').str.replace(' +', ' ').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9dcfbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"text\": 0, \"clean\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08711808",
   "metadata": {},
   "source": [
    "Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d22b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1] = df.apply(lambda row: nltk.word_tokenize(row[1]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de2cfbb",
   "metadata": {},
   "source": [
    "Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "419605f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45785658",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l','m','n','o','p','q','r','s','t', 'u', 'v', 'w', 'x', 'y', 'z', \"about\", \"across\", \"after\", \"all\", \"also\", \"an\", \"and\", \"another\", \"added\",\n",
    "\"any\", \"are\", \"as\", \"at\", \"basically\", \"be\", \"because\", 'become', \"been\", \"before\", \"being\", \"between\",\"both\", \"but\", \"by\",\"came\",\"can\",\"come\",\"could\",\"did\",\"do\",\"does\",\"each\",\"else\",\"every\",\"either\",\"especially\", \"for\",\"from\",\"get\",\"given\",\"gets\",\n",
    "'give','gives',\"got\",\"goes\",\"had\",\"has\",\"have\",\"he\",\"her\",\"here\",\"him\",\"himself\",\"his\",\"how\",\"if\",\"in\",\"into\",\"is\",\"it\",\"its\",\"just\",\"lands\",\"like\",\"make\",\"making\", \"made\", \"many\",\"may\",\"me\",\"might\",\"more\",\"most\",\"much\",\"must\",\"my\",\"never\",\"provide\", \n",
    "\"provides\", \"perhaps\",\"no\",\"now\",\"of\",\"on\",\"only\",\"or\",\"other\", \"our\",\"out\",\"over\",\"re\",\"said\",\"same\",\"see\",\"should\",\"since\",\"so\",\"some\",\"still\",\"such\",\"seeing\", \"see\", \"take\",\"than\",\"that\",\"the\",\"their\",\"them\",\"then\",\"there\",\n",
    "\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"up\",\"use\",\"using\",\"used\", \"underway\", \"very\",\"want\",\"was\",\"way\",\"we\",\"well\",\"were\",\"what\",\"when\",\"where\",\"which\",\"while\",\"whilst\",\"who\",\"will\",\"with\",\"would\",\"you\",\"your\", \n",
    "'etc', 'via', 'eg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f5a756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words += ['hi','\\n','\\n\\n', '&amp;', ' ', '.', '-', 'got', \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'rt', 'feel', 'give', 'giving', 'help', 'said', 'also', 'gave', 'like', 'going', 'even']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24c055d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1] = df[1].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576707b",
   "metadata": {},
   "source": [
    "Perform Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f5e2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df[1] = df[1].apply(lambda x: [wordnet_lemmatizer.lemmatize(y) for y in x]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2fdb3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df[1].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f9921",
   "metadata": {},
   "source": [
    "## Create a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55ee51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of all words in all documents\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "# filter extreme cases out of dictionary\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# create BOW dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077857a2",
   "metadata": {},
   "source": [
    "## Create LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c76dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LDA model using preferred hyperparameters\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                         num_topics=5, \n",
    "                                         id2word=dictionary, \n",
    "                                         passes=4, \n",
    "                                         workers=2,\n",
    "                                         random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e715980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b02f6f",
   "metadata": {},
   "source": [
    "View LDA topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08e0e701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.037*\"good\" + 0.036*\"done\" + 0.029*\"new\" + 0.029*\"time\" + 0.023*\"best\" + 0.022*\"na\" + 0.021*\"gon\" + 0.021*\"think\" + 0.020*\"shit\" + 0.019*\"year\"'),\n",
       " (1,\n",
       "  '0.028*\"first\" + 0.027*\"morning\" + 0.027*\"need\" + 0.026*\"good\" + 0.024*\"work\" + 0.022*\"day\" + 0.018*\"know\" + 0.018*\"looking\" + 0.017*\"people\" + 0.016*\"hope\"'),\n",
       " (2,\n",
       "  '0.055*\"love\" + 0.055*\"one\" + 0.031*\"day\" + 0.028*\"thank\" + 0.024*\"really\" + 0.022*\"year\" + 0.019*\"girl\" + 0.018*\"people\" + 0.016*\"show\" + 0.015*\"happy\"'),\n",
       " (3,\n",
       "  '0.037*\"one\" + 0.032*\"go\" + 0.021*\"oh\" + 0.018*\"need\" + 0.018*\"lmao\" + 0.017*\"life\" + 0.016*\"getting\" + 0.015*\"let\" + 0.015*\"buy\" + 0.015*\"god\"'),\n",
       " (4,\n",
       "  '0.041*\"amp\" + 0.027*\"time\" + 0.025*\"know\" + 0.023*\"say\" + 0.022*\"live\" + 0.021*\"na\" + 0.020*\"people\" + 0.019*\"better\" + 0.018*\"yeah\" + 0.017*\"back\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700f0562",
   "metadata": {},
   "source": [
    "### Calculate LDA Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8601510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4204149214638339\n"
     ]
    }
   ],
   "source": [
    "cm = CoherenceModel(model=lda_model, corpus=bow_corpus, texts=docs, coherence='c_v')\n",
    "coherence_lda = cm.get_coherence() \n",
    "print(coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05ddef",
   "metadata": {},
   "source": [
    "## Create GSDMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46d9e56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 4357 clusters with 15 clusters populated\n",
      "In stage 1: transferred 3389 clusters with 15 clusters populated\n",
      "In stage 2: transferred 3107 clusters with 15 clusters populated\n",
      "In stage 3: transferred 2923 clusters with 15 clusters populated\n",
      "In stage 4: transferred 2803 clusters with 15 clusters populated\n",
      "In stage 5: transferred 2788 clusters with 15 clusters populated\n",
      "In stage 6: transferred 2747 clusters with 15 clusters populated\n",
      "In stage 7: transferred 2724 clusters with 15 clusters populated\n",
      "In stage 8: transferred 2679 clusters with 15 clusters populated\n",
      "In stage 9: transferred 2652 clusters with 15 clusters populated\n",
      "In stage 10: transferred 2653 clusters with 15 clusters populated\n",
      "In stage 11: transferred 2640 clusters with 15 clusters populated\n",
      "In stage 12: transferred 2606 clusters with 15 clusters populated\n",
      "In stage 13: transferred 2638 clusters with 15 clusters populated\n",
      "In stage 14: transferred 2621 clusters with 15 clusters populated\n"
     ]
    }
   ],
   "source": [
    "# create variable containing length of dictionary/vocab\n",
    "vocab_length = len(dictionary)\n",
    "\n",
    "# initialize GSDMM\n",
    "gsdmm = MovieGroupProcess(K=15, alpha=0.1, beta=0.3, n_iters=15)\n",
    "\n",
    "# fit GSDMM model\n",
    "y = gsdmm.fit(docs, vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c013830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2442b9",
   "metadata": {},
   "source": [
    "Display GSDMM topics with top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df17758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic : [335 439 335 335 439 297 464 378 281 416 219 194 273 300 296]\n",
      "Most important clusters (by number of docs inside): [ 6  4  1  9  7  3  2  0 13  5 14  8 12 10 11]\n",
      "\n",
      "Cluster 6 : [('oh', 44), ('love', 41), ('thank', 41), ('man', 24), ('happy', 23), ('god', 23), ('beautiful', 19), ('omg', 16), ('best', 16), ('birthday', 15), ('photo', 14), ('today', 13), ('lt', 13), ('look', 13), ('day', 12), ('baby', 12), ('posted', 11), ('literally', 11), ('wait', 11), ('go', 10)]\n",
      "\n",
      "Cluster 4 : [('na', 96), ('gon', 48), ('wan', 47), ('go', 30), ('back', 24), ('time', 23), ('lol', 19), ('good', 19), ('love', 18), ('let', 18), ('ta', 17), ('play', 16), ('think', 14), ('keep', 13), ('mean', 13), ('really', 13), ('buy', 13), ('life', 12), ('anything', 12), ('man', 12)]\n",
      "\n",
      "Cluster 1 : [('morning', 64), ('good', 52), ('day', 44), ('one', 30), ('nice', 22), ('night', 21), ('today', 18), ('vote', 18), ('friend', 18), ('elonmusk', 18), ('love', 16), ('please', 15), ('amp', 14), ('last', 13), ('people', 13), ('know', 13), ('thing', 13), ('first', 11), ('lol', 11), ('omicron', 11)]\n",
      "\n",
      "Cluster 9 : [('done', 52), ('luck', 28), ('good', 25), ('go', 23), ('start', 18), ('getting', 17), ('live', 16), ('lol', 14), ('new', 14), ('life', 14), ('guy', 13), ('need', 11), ('week', 10), ('back', 10), ('better', 10), ('day', 9), ('wrong', 9), ('hope', 9), ('let', 9), ('damn', 9)]\n",
      "\n",
      "Cluster 7 : [('know', 28), ('people', 24), ('say', 21), ('lol', 19), ('bro', 18), ('thing', 17), ('shit', 17), ('need', 17), ('amp', 17), ('black', 17), ('one', 15), ('really', 15), ('man', 15), ('as', 15), ('oh', 14), ('hell', 14), ('character', 13), ('always', 13), ('damn', 12), ('think', 11)]\n",
      "\n",
      "Cluster 3 : [('time', 36), ('new', 27), ('day', 26), ('game', 25), ('one', 25), ('gt', 22), ('work', 19), ('amp', 17), ('really', 16), ('play', 15), ('next', 14), ('yet', 14), ('month', 13), ('watch', 12), ('love', 12), ('good', 11), ('thing', 11), ('something', 11), ('great', 10), ('first', 10)]\n",
      "\n",
      "Cluster 2 : [('time', 34), ('one', 32), ('people', 25), ('bad', 21), ('say', 20), ('need', 16), ('look', 15), ('thing', 13), ('little', 12), ('know', 12), ('good', 11), ('show', 11), ('think', 11), ('big', 11), ('great', 11), ('friend', 10), ('anyone', 10), ('tweet', 10), ('away', 10), ('sure', 10)]\n",
      "\n",
      "Cluster 0 : [('one', 32), ('know', 31), ('think', 19), ('best', 17), ('love', 16), ('link', 15), ('time', 15), ('really', 13), ('far', 12), ('yeah', 12), ('guy', 12), ('literally', 11), ('back', 11), ('work', 11), ('part', 11), ('people', 10), ('day', 10), ('someone', 10), ('class', 10), ('person', 10)]\n",
      "\n",
      "Cluster 13 : [('love', 27), ('year', 25), ('one', 24), ('enhypen', 17), ('u', 15), ('enhypen_members', 13), ('happy', 13), ('need', 12), ('vaccine', 12), ('butter', 12), ('dynamite', 12), ('know', 11), ('10', 11), ('better', 9), ('boy', 9), ('people', 9), ('100', 9), ('covid', 8), ('new', 8), ('happy1stenniversary', 8)]\n",
      "\n",
      "Cluster 5 : [('amp', 37), ('thanks', 29), ('first', 20), ('go', 15), ('one', 15), ('v', 14), ('live', 12), ('day', 12), ('know', 11), ('work', 11), ('game', 10), ('year', 10), ('big', 10), ('start', 10), ('team', 9), ('better', 9), ('student', 9), ('mom', 9), ('home', 8), ('someone', 8)]\n",
      "\n",
      "Cluster 14 : [('one', 23), ('think', 20), ('year', 20), ('people', 19), ('time', 18), ('school', 13), ('right', 11), ('please', 9), ('stop', 9), ('fan', 9), ('let', 9), ('know', 9), ('account', 9), ('old', 8), ('ever', 8), ('number', 8), ('state', 8), ('yes', 8), ('parent', 8), ('yeah', 7)]\n",
      "\n",
      "Cluster 8 : [('00', 19), ('2021', 14), ('year', 12), ('day', 12), ('time', 11), ('30', 10), ('dm', 10), ('11', 9), ('wind', 9), ('november', 9), ('yes', 8), ('pm', 8), ('10', 8), ('started', 8), ('u', 7), ('today', 7), ('check', 6), ('morning', 6), ('ready', 6), ('good', 6)]\n",
      "\n",
      "Cluster 12 : [('question', 15), ('check', 13), ('need', 13), ('people', 12), ('woman', 12), ('case', 9), ('open', 8), ('trying', 8), ('read', 8), ('guy', 7), ('word', 7), ('power', 7), ('claim', 7), ('put', 7), ('first', 7), ('house', 7), ('around', 7), ('let', 6), ('last', 6), ('know', 6)]\n",
      "\n",
      "Cluster 10 : [('valo', 14), ('point', 13), ('grade', 10), ('8th', 10), ('monday', 10), ('end', 8), ('black', 8), ('right', 7), ('year', 7), ('top', 7), ('week', 7), ('last', 7), ('def', 6), ('time', 6), ('player', 6), ('mission', 6), ('messi', 6), ('sale', 6), ('new', 6), ('cyber', 6)]\n",
      "\n",
      "Cluster 11 : [('project', 19), ('amp', 16), ('thank', 13), ('really', 12), ('good', 11), ('virus', 9), ('find', 8), ('thanks', 7), ('sign', 7), ('play', 6), ('back', 6), ('big', 6), ('check', 5), ('man', 5), ('long', 5), ('remember', 5), ('company', 5), ('airdrop', 5), ('vaccine', 5), ('successful', 5)]\n"
     ]
    }
   ],
   "source": [
    "# print number of documents per topic\n",
    "doc_count = np.array(gsdmm.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "\n",
    "# Topics sorted by the number of document they are allocated to\n",
    "top_index = doc_count.argsort()[-15:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "\n",
    "# define function to get top words per topic\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts = sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print(\"\\nCluster %s : %s\"%(cluster, sort_dicts))\n",
    "\n",
    "# get top words in topics\n",
    "top_words(gsdmm.cluster_word_distribution, top_index, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be93cd",
   "metadata": {},
   "source": [
    "Create Lists from GSDMM topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ea492c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_lists(model, top_clusters, n_words):\n",
    "    '''\n",
    "    Gets lists of words in topics as a list of lists.\n",
    "    \n",
    "    model: gsdmm instance\n",
    "    top_clusters:  numpy array containing indices of top_clusters\n",
    "    n_words: top n number of words to include\n",
    "    \n",
    "    '''\n",
    "    # create empty list to contain topics\n",
    "    topics = []\n",
    "    \n",
    "    # iterate over top n clusters\n",
    "    for cluster in top_clusters:\n",
    "        #create sorted dictionary of word distributions\n",
    "        sorted_dict = sorted(model.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:n_words]\n",
    "         \n",
    "        #create empty list to contain words\n",
    "        topic = []\n",
    "        \n",
    "        #iterate over top n words in topic\n",
    "        for k,v in sorted_dict:\n",
    "            #append words to topic list\n",
    "            topic.append(k)\n",
    "            \n",
    "        #append topics to topics list    \n",
    "        topics.append(topic)\n",
    "    \n",
    "    return topics\n",
    "\n",
    "# get topics to feed to coherence model\n",
    "topics = get_topics_lists(gsdmm, top_index, 20) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20da1be",
   "metadata": {},
   "source": [
    "### Calculate GSDMM Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ae2c0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3232805025489422\n"
     ]
    }
   ],
   "source": [
    "# evaluate model using Topic Coherence score\n",
    "cm_gsdmm = CoherenceModel(topics=topics, dictionary=dictionary, corpus=bow_corpus, texts=docs, coherence='c_v')\n",
    "\n",
    "# get coherence value\n",
    "coherence_gsdmm = cm_gsdmm.get_coherence()  \n",
    "\n",
    "print(coherence_gsdmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "133e04e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 6 : [('oh', 44), ('love', 41), ('thank', 41), ('man', 24), ('happy', 23), ('god', 23), ('beautiful', 19), ('omg', 16), ('best', 16), ('birthday', 15), ('photo', 14), ('today', 13), ('lt', 13), ('look', 13), ('day', 12), ('baby', 12), ('posted', 11), ('literally', 11), ('wait', 11), ('go', 10)]\n",
      "\n",
      "Cluster 4 : [('na', 96), ('gon', 48), ('wan', 47), ('go', 30), ('back', 24), ('time', 23), ('lol', 19), ('good', 19), ('love', 18), ('let', 18), ('ta', 17), ('play', 16), ('think', 14), ('keep', 13), ('mean', 13), ('really', 13), ('buy', 13), ('life', 12), ('anything', 12), ('man', 12)]\n",
      "\n",
      "Cluster 1 : [('morning', 64), ('good', 52), ('day', 44), ('one', 30), ('nice', 22), ('night', 21), ('today', 18), ('vote', 18), ('friend', 18), ('elonmusk', 18), ('love', 16), ('please', 15), ('amp', 14), ('last', 13), ('people', 13), ('know', 13), ('thing', 13), ('first', 11), ('lol', 11), ('omicron', 11)]\n",
      "\n",
      "Cluster 9 : [('done', 52), ('luck', 28), ('good', 25), ('go', 23), ('start', 18), ('getting', 17), ('live', 16), ('lol', 14), ('new', 14), ('life', 14), ('guy', 13), ('need', 11), ('week', 10), ('back', 10), ('better', 10), ('day', 9), ('wrong', 9), ('hope', 9), ('let', 9), ('damn', 9)]\n",
      "\n",
      "Cluster 7 : [('know', 28), ('people', 24), ('say', 21), ('lol', 19), ('bro', 18), ('thing', 17), ('shit', 17), ('need', 17), ('amp', 17), ('black', 17), ('one', 15), ('really', 15), ('man', 15), ('as', 15), ('oh', 14), ('hell', 14), ('character', 13), ('always', 13), ('damn', 12), ('think', 11)]\n",
      "\n",
      "Cluster 3 : [('time', 36), ('new', 27), ('day', 26), ('game', 25), ('one', 25), ('gt', 22), ('work', 19), ('amp', 17), ('really', 16), ('play', 15), ('next', 14), ('yet', 14), ('month', 13), ('watch', 12), ('love', 12), ('good', 11), ('thing', 11), ('something', 11), ('great', 10), ('first', 10)]\n",
      "\n",
      "Cluster 2 : [('time', 34), ('one', 32), ('people', 25), ('bad', 21), ('say', 20), ('need', 16), ('look', 15), ('thing', 13), ('little', 12), ('know', 12), ('good', 11), ('show', 11), ('think', 11), ('big', 11), ('great', 11), ('friend', 10), ('anyone', 10), ('tweet', 10), ('away', 10), ('sure', 10)]\n",
      "\n",
      "Cluster 0 : [('one', 32), ('know', 31), ('think', 19), ('best', 17), ('love', 16), ('link', 15), ('time', 15), ('really', 13), ('far', 12), ('yeah', 12), ('guy', 12), ('literally', 11), ('back', 11), ('work', 11), ('part', 11), ('people', 10), ('day', 10), ('someone', 10), ('class', 10), ('person', 10)]\n",
      "\n",
      "Cluster 13 : [('love', 27), ('year', 25), ('one', 24), ('enhypen', 17), ('u', 15), ('enhypen_members', 13), ('happy', 13), ('need', 12), ('vaccine', 12), ('butter', 12), ('dynamite', 12), ('know', 11), ('10', 11), ('better', 9), ('boy', 9), ('people', 9), ('100', 9), ('covid', 8), ('new', 8), ('happy1stenniversary', 8)]\n",
      "\n",
      "Cluster 5 : [('amp', 37), ('thanks', 29), ('first', 20), ('go', 15), ('one', 15), ('v', 14), ('live', 12), ('day', 12), ('know', 11), ('work', 11), ('game', 10), ('year', 10), ('big', 10), ('start', 10), ('team', 9), ('better', 9), ('student', 9), ('mom', 9), ('home', 8), ('someone', 8)]\n",
      "\n",
      "Cluster 14 : [('one', 23), ('think', 20), ('year', 20), ('people', 19), ('time', 18), ('school', 13), ('right', 11), ('please', 9), ('stop', 9), ('fan', 9), ('let', 9), ('know', 9), ('account', 9), ('old', 8), ('ever', 8), ('number', 8), ('state', 8), ('yes', 8), ('parent', 8), ('yeah', 7)]\n",
      "\n",
      "Cluster 8 : [('00', 19), ('2021', 14), ('year', 12), ('day', 12), ('time', 11), ('30', 10), ('dm', 10), ('11', 9), ('wind', 9), ('november', 9), ('yes', 8), ('pm', 8), ('10', 8), ('started', 8), ('u', 7), ('today', 7), ('check', 6), ('morning', 6), ('ready', 6), ('good', 6)]\n",
      "\n",
      "Cluster 12 : [('question', 15), ('check', 13), ('need', 13), ('people', 12), ('woman', 12), ('case', 9), ('open', 8), ('trying', 8), ('read', 8), ('guy', 7), ('word', 7), ('power', 7), ('claim', 7), ('put', 7), ('first', 7), ('house', 7), ('around', 7), ('let', 6), ('last', 6), ('know', 6)]\n",
      "\n",
      "Cluster 10 : [('valo', 14), ('point', 13), ('grade', 10), ('8th', 10), ('monday', 10), ('end', 8), ('black', 8), ('right', 7), ('year', 7), ('top', 7), ('week', 7), ('last', 7), ('def', 6), ('time', 6), ('player', 6), ('mission', 6), ('messi', 6), ('sale', 6), ('new', 6), ('cyber', 6)]\n",
      "\n",
      "Cluster 11 : [('project', 19), ('amp', 16), ('thank', 13), ('really', 12), ('good', 11), ('virus', 9), ('find', 8), ('thanks', 7), ('sign', 7), ('play', 6), ('back', 6), ('big', 6), ('check', 5), ('man', 5), ('long', 5), ('remember', 5), ('company', 5), ('airdrop', 5), ('vaccine', 5), ('successful', 5)]\n"
     ]
    }
   ],
   "source": [
    "top_words(gsdmm.cluster_word_distribution, top_index, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5df91040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3232805025489422\n"
     ]
    }
   ],
   "source": [
    "cm_gsdmm = CoherenceModel(topics=topics, dictionary=dictionary, corpus=bow_corpus, texts=docs, coherence='c_v')\n",
    "coherence_gsdmm = cm_gsdmm.get_coherence()  \n",
    "print(coherence_gsdmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d44d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
